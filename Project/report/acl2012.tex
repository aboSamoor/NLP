%
% File acl2012.tex
%
% Contact: Maggie Li (cswjli@comp.polyu.edu.hk), Michael White (mwhite@ling.osu.edu)
%%
%% Based on the style files for ACL2008 by Joakim Nivre and Noah Smith
%% and that of ACL2010 by Jing-Shin Chang and Philipp Koehn


\documentclass[11pt]{article}
\usepackage{acl2012}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{url}
\DeclareMathOperator*{\argmax}{arg\,max}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{Detecting English Writing Styles For Non Native Speakers}

\author{Rami Al-Rfou', Yanging Chen, Yejin Choi \\
  Department of Computer Science \\
  Stony Brook University \\
  NY 11794, USA \\
  {\tt \{ralrfou, cyanqing, ychoi\}@cs.stonybrook.edu}}

\date{12/15/2011}

\usepackage{graphicx}
\begin{document}
\maketitle
\begin{abstract}
%Rami's Part
%Explain what is our target of whole project.
\end{abstract}


\section{Introduction}
%Rami's Part

\section{Related Work}
%Chen's part
%Mentions what people did, and how our work is different and original.


\section{Wikipedia}
%Explain why wikipedia is an awesome source
%Explain wikipedia structure
%Explain how there are two different ways to get users contributions
%Explain the comments extraction algorithm
%Rami's Part

Wikipedia is the de facto source of knowledge for internet users. Wikipedia is the 5\textsuperscript{th} most popular website according to Google ranking. For researchers Wikipedia is a giant linguistic and social jar of experiments. The richness of the website content that is written by users from different backgrounds represents a robust sample of the current languages usage by native and non native speakers.

With more than 90 thousand active users and 4.4 million article the content of Wikipedia spans large number of topics. The diversity of the authors of those articles beside the records of the revisions that are stored in a database of revisions that the website offer for free presents a realistic source of text. Such resource presents a higher quality of data that is not achievable by the other commonly used sources of text as news and scientific papers.

Such successful website has a complex database structure to serve its users. Therefore, extracting data could be a complex process. Our goal is to identify the languages skills of the users and collect their contributions. To achieve the first task Wikipedia has a an information box called \emph{Babel} that users can add voluntarily to their profile pages to state their skills in different languages. Figure \ref{babel} shows a user who identified his native language and her skills in 3 other non native languages in a scale for 1-5. This info box will be indexed in the database as categories.


\begin{figure}[htp]
\centering
\includegraphics[scale=0.60]{babel}
\caption{Wikipedia languages skills info box (Babel)}
\label{babel}
\end{figure}

To collect the contributions of a specific user the task is more complex procedure. The diffs between Wikipedia pages revisions has to be generated and linked back to the user table. However, the resources we have to process such huge amount of data did not allow us to do that\footnote{Recent efforts were made to generate the diffs \url{http://dumps.wikimedia.org/other/diffdb/}}. Instead we noticed that Wikipedia pages have accompanying discussion pages where users discuss different aspects of the articles. In those pages the tradition is to sign the user comments with a signature that link back to the user. Figure \ref{obama} shows the style of the writing of those talk pages are less formal and technical than the main pages of Wikipedia and has more conversational stylistic features.

\begin{figure*}[Htp]
\centering
\includegraphics[scale=0.285]{obama.png}
\caption{Example of a conversation in the discussion pages}
\label{obama}
\end{figure*}

The patterns of the recommended signatures styles are limited in number, however, in practice the users use various patterns that makes the detection rules ambiguous. The detection algorithm implemented relies on complex regular expressions and applies best effort strategy.



\section{Experiments}
%Rami's part
We found that around 60 thousands users specified their language skills. Figure \ref{native_dist} shows that the percentage of users who claimed that their one of their native languages is English is around 47\% of English wikipedia users base. 
\begin{figure}[htp]
\centering
\includegraphics[scale=0.5]{chart_4.png}
\caption{Users distribution over native languages in English Wikipedia}
\label{native_dist}
\end{figure}

We parsed the talk pages with the \verb+namespace=1+, they represents x\% of the talk pages, which produced around 12 million comments. Only 2.4 million comment we could identify to users with known language skills. Moreover, not all the users made comments in the talk pages we parsed, therefore, The number of the users who made at least one contribution in the extracted contributions is around 30 thousand user.

As we have large number of comments and users and as we believe the data we have is still noisy. We applied the following filtering mechanisms:
\begin{itemize}
\item We picked the users of the most popular 20 native languages.
\item We picked out of the English native speakers the users who specified the EN-US as their native language only to avoid users who are so skilled in English but are not living in English speaking countries.
\item We excluded the users who specified more than one native language out of the picked native languages to avoid unrealistic scenarios where users claim to be native in more than two languages.
\end{itemize}

The new data set after the filtering is consistent of 9857 user and 589228 comments.

\subsection{Setup}
%Explain the pruning and the filtering that was done to the data.
The following experiments are conducted under the following conditions:
\begin{itemize}
\item The accepted comments has to have at least 20 tokens to avoid short and non meaningful comments.
\item Proper nouns are replaced by their tags to avoid bias toward topics.
\item Non ascii characters are replaced by a special character to avoid bias foreign languages usage in the comments.
\item The classifier has balanced number of samples for each its classes.
\item Logistic Regression algorithm is used to for the classification task.
\item The data set is split to 70\% training set, 10\% development set and 20\% testing set.
\end{itemize}

\subsection{Features}
The comments of training set is grouped by class and the following language models are calculated for each class:
\begin{itemize}
\item
\end{itemize}

We calculated the 1-4 grams over the words


\subsection{Popular Languages Experiment}
%Explain the experiment and the results

\begin{figure}[htp]
\centering
\includegraphics[scale=0.45]{popular_cfm.png}
\caption{Popular Languages experiment confusion matrix}
\label{pop_cfm}
\end{figure}

\begin{figure}[htp]
\centering
\includegraphics[scale=0.45]{popular_lc.png}
\caption{Popular languages experiment learning curves}
\label{pop_lc}
\end{figure}
		
\subsection{Languages Families Experiment}
%Explain the experiment and the results
\begin{figure}[htp]
\centering
\includegraphics[scale=0.45]{family_cfm.png}
\caption{Languages families experiment confusion matrix}
\label{fam_cfm}
\end{figure}

\begin{figure}[htp]
\centering
\includegraphics[scale=0.45]{family_lc.png}
\caption{Languages families experiment learning curves}
\label{fam_lc}
\end{figure}

\subsection{Native vs Non Native Experiment}
%Explain the experiment and the results
\begin{figure}[htp]
\centering
\includegraphics[scale=0.45]{native_cfm.png}
\caption{Native vs Non Native speakers experiment confusion matrix}
\label{non_cfm}
\end{figure}

\begin{figure}[htp]
\centering
\includegraphics[scale=0.45]{native_lc.png}
\caption{Non native vs native experiment learning curves}
\label{non_lc}
\end{figure}

\section{Writing Styles}
%Chen's part


\section{Conclusions}
%Chen's Part

\section*{Acknowledgments}
%Chen's Part
%We should thank Prof. Skiena for the resources he gave us.

\begin{thebibliography}{}

\bibitem[\protect\citename{Aho and Ullman}1972]{Aho:72}
Alfred~V. Aho and Jeffrey~D. Ullman.
\newblock 1972.
\newblock {\em The Theory of Parsing, Translation and Compiling}, volume~1.
\newblock Prentice-{Hall}, Englewood Cliffs, NJ.

\bibitem[\protect\citename{{American Psychological Association}}1983]{APA:83}
{American Psychological Association}.
\newblock 1983.
\newblock {\em Publications Manual}.
\newblock American Psychological Association, Washington, DC.

\bibitem[\protect\citename{{Association for Computing Machinery}}1983]{ACM:83}
{Association for Computing Machinery}.
\newblock 1983.
\newblock {\em Computing Reviews}, 24(11):503--512.

\bibitem[\protect\citename{Chandra \bgroup et al.\egroup }1981]{Chandra:81}
Ashok~K. Chandra, Dexter~C. Kozen, and Larry~J. Stockmeyer.
\newblock 1981.
\newblock Alternation.
\newblock {\em Journal of the Association for Computing Machinery},
  28(1):114--133.

\bibitem[\protect\citename{Gusfield}1997]{Gusfield:97}
Dan Gusfield.
\newblock 1997.
\newblock {\em Algorithms on Strings, Trees and Sequences}.
\newblock Cambridge University Press, Cambridge, UK.

\end{thebibliography}

\end{document}
